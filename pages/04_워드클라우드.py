# app.py
import streamlit as st
import requests
from urllib.parse import urlparse, parse_qs
import re
from collections import Counter
from io import BytesIO
import tempfile
import os
import matplotlib.pyplot as plt
from matplotlib import font_manager
from wordcloud import WordCloud, STOPWORDS as WC_STOPWORDS

# ============================ í˜ì´ì§€ ì„¤ì • ============================
st.set_page_config(page_title="ğŸˆ ìœ íŠœë¸Œ ëŒ“ê¸€ ì›Œë“œí´ë¼ìš°ë“œ", layout="wide")
st.title("ğŸˆ ìœ íŠœë¸Œ ëŒ“ê¸€ ì›Œë“œí´ë¼ìš°ë“œ")
st.caption("ğŸ§© ì£¼ì†Œë¥¼ ë„£ê³  ìµœëŒ€ ëŒ“ê¸€ ìˆ˜ë¥¼ ê³ ë¥¸ ë’¤ ë²„íŠ¼ì„ ëˆŒëŸ¬ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ë§Œë“¤ì–´ë´ìš”.")

DEFAULT_URL = "https://www.youtube.com/watch?v=WXuK6gekU1Y"

# ============================ ìœ í‹¸ í•¨ìˆ˜ë“¤ ============================
def extract_video_id(url: str):
    """ì—¬ëŸ¬ í˜•íƒœì˜ ìœ íŠœë¸Œ ì£¼ì†Œì—ì„œ ì˜ìƒ IDë¥¼ ë½‘ì•„ìš”."""
    if not url:
        return None
    url = url.strip()
    if not re.match(r"^https?://", url, flags=re.I):
        url = "https://" + url
    try:
        parsed = urlparse(url)
    except Exception:
        return None

    host = (parsed.netloc or "").lower()
    path = parsed.path or ""
    qs = parse_qs(parsed.query or "")

    if "youtube.com" in host:
        if "v" in qs and qs["v"]:
            return qs["v"][0]
        m = re.search(r"/embed/([A-Za-z0-9_-]{6,})", path)
        if m:
            return m.group(1)
        m = re.search(r"/shorts/([A-Za-z0-9_-]{6,})", path)
        if m:
            return m.group(1)
        m = re.search(r"/live/([A-Za-z0-9_-]{6,})", path)
        if m:
            return m.group(1)
    if "youtu.be" in host:
        m = re.match(r"^/([A-Za-z0-9_-]{6,})", path)
        if m:
            return m.group(1)
    if "vi" in qs and qs["vi"]:
        return qs["vi"][0]
    return None


@st.cache_resource(show_spinner=False)
def get_session() -> requests.Session:
    """API í˜¸ì¶œìš© ì„¸ì…˜ì„ í•œ ë²ˆ ë§Œë“¤ê³  ì¬ì‚¬ìš©í•´ìš”."""
    s = requests.Session()
    s.headers.update({
        "Accept": "application/json, text/plain, */*",
        "User-Agent": "Mozilla/5.0 (WordcloudApp/1.0)"
    })
    return s


def raise_api_error(resp: requests.Response) -> None:
    """API ì˜¤ë¥˜ë¥¼ ì‚¬ëŒì´ ì´í•´í•˜ê¸° ì‰¬ìš´ ì˜ˆì™¸ë¡œ ë°”ê¿”ìš”."""
    try:
        data = resp.json()
        reason = data.get("error", {}).get("errors", [{}])[0].get("reason", "")
        code = data.get("error", {}).get("code", resp.status_code)
    except Exception:
        reason = ""
        code = resp.status_code
    raise RuntimeError(f"{code}:{reason}")


def fetch_video_title(api_key: str, video_id: str, session: requests.Session) -> str:
    """ì˜ìƒ ì œëª©ì„ ê°€ì ¸ì™€ìš”(íŒŒì¼ëª…ì— ì“¸ ê±°ì˜ˆìš”)."""
    url = "https://www.googleapis.com/youtube/v3/videos"
    params = {"id": video_id, "part": "snippet", "key": api_key}
    resp = session.get(url, params=params, timeout=20)
    if resp.status_code != 200:
        raise_api_error(resp)
    items = resp.json().get("items", [])
    if not items:
        raise RuntimeError("404:notFound")
    return items[0]["snippet"]["title"] or "video"


def fetch_comment_texts(api_key: str, video_id: str, session: requests.Session, max_count: int) -> list[str]:
    """ì¸ê¸°ìˆœìœ¼ë¡œ ëŒ“ê¸€ ë³¸ë¬¸ì„ ìµœëŒ€ max_countê°œ ëª¨ì•„ìš”."""
    url = "https://www.googleapis.com/youtube/v3/commentThreads"
    params = {
        "part": "snippet",
        "videoId": video_id,
        "key": api_key,
        "order": "relevance",
        "maxResults": 100,
        "textFormat": "plainText",
    }
    texts: list[str] = []
    page_token = None
    while True:
        if page_token:
            params["pageToken"] = page_token
        else:
            params.pop("pageToken", None)
        resp = session.get(url, params=params, timeout=20)
        if resp.status_code != 200:
            raise_api_error(resp)
        data = resp.json()
        for it in data.get("items", []):
            try:
                sn = it["snippet"]["topLevelComment"]["snippet"]
                txt = (sn.get("textDisplay", "") or "").replace("\n", " ").strip()
                if txt:
                    texts.append(txt)
            except Exception:
                continue
            if len(texts) >= max_count:
                return texts
        page_token = data.get("nextPageToken")
        if not page_token:
            break
    return texts


def sanitize_filename(name: str) -> str:
    """íŒŒì¼ëª…ì— ì“¸ ìˆ˜ ì—†ëŠ” ë¬¸ìë¥¼ ì œê±°í•´ìš”."""
    name = re.sub(r"[\\/*?:\"<>|]", "", name)
    name = re.sub(r"\s+", " ", name).strip()
    return name or "wordcloud"


def tokenize_clean(text: str) -> list[str]:
    """íŠ¹ìˆ˜ë¬¸ì/ì´ëª¨ì§€ ì œê±°, 2ê¸€ì ì´ìƒ í•œ/ì˜/ìˆ«ì ë‹¨ì–´ë§Œ ë‚¨ê²¨ìš”."""
    cleaned = re.sub(r"[^ê°€-í£A-Za-z0-9\s]", " ", text)
    cleaned = cleaned.lower()
    tokens = re.findall(r"[ê°€-í£a-z0-9]+", cleaned)
    return [t for t in tokens if len(t) >= 2]


# --------- í•œê¸€ ê¸°ë³¸ ë¶ˆìš©ì–´(ë„‰ë„‰í•˜ê²Œ, ì¡°ì‚¬/ëŒ€ëª…ì‚¬/ì¶”ì„ìƒˆ/ìƒíˆ¬ì–´ í¬í•¨) ----------
BASE_KO_STOPWORDS = {
    "ê·¸ë¦¬ê³ ","ê·¸ëŸ¬ë‚˜","í•˜ì§€ë§Œ","ê·¸ë˜ì„œ","ë˜í•œ","ë°","ë“±","ë˜","ì•„ë‹ˆë¼","ë³´ë‹¤","ìœ„í•´","ëŒ€í•œ","ë•Œë¬¸","ë•Œë¬¸ì—","ìœ¼ë¡œ","ìœ¼ë¡œì¨","ìœ¼ë¡œì„œ","ì—ì„œ",
    "ì—ê²Œ","ì—ê²Œì„œ","ë¶€í„°","ê¹Œì§€","ì´ë‹¤","ë˜ë‹¤","í•˜ë‹¤","í•©ë‹ˆë‹¤","í•´ìš”","í•œë‹¤","í–ˆë‹¤","í•˜ëŠ”","í•˜ë©´","í•˜ë©°","í•˜ì—¬","í•˜ë‹ˆ","í•˜ê³ ",
    "ë©ë‹ˆë‹¤","ë˜ëŠ”","ë˜ì–´","ëë‹¤","ìˆë‹¤","ì—†ë‹¤","ê°™ë‹¤","ìˆ˜","ê²ƒ","ê±°","ë“¤","ê·¸","ì´","ì €","ê·¸ê²ƒ","ì´ê²ƒ","ì €ê²ƒ","ë•Œ","ì¢€","ì•„ì£¼","ë„ˆë¬´","ë§¤ìš°",
    "ì§„ì§œ","ì •ë§","ê·¸ëƒ¥","ì•„ë§ˆ","ì´ë¯¸","ë‹¤ì‹œ","ë‹¤ë¥¸","ìµœê·¼","ì²˜ëŸ¼","ê°™ì´","ìš°ë¦¬","ì €í¬","ë‚´","ë‚´ê°€","ë‚˜","ë„ˆ","ë‹ˆ","ë‹ˆê°€","ë„ˆê°€","ê·¸ë…€",
    "ê·¸ëŠ”","ê·¸ë…€ëŠ”","ì €ëŠ”","ë‚˜ëŠ”","ìš°ë¦¬ëŠ”","ì—¬ëŸ¬ë¶„","ì˜¤ëŠ˜","ì˜ìƒ","ëŒ“ê¸€","ìœ íŠœë¸Œ","ë³´ê¸°","ìš”","ë„¤","ì£ ","ìš”ì¦˜","ê±°ì˜","í˜„ì¬","ê·¸ê²Œ","ì´ê²Œ","ì €ê²Œ",
    "ë­”ê°€","ë­","ë­”","ì´ëŸ°","ì €ëŸ°","ê·¸ëŸ°","ë­”ì§€","ì–´ë–¤","ë¬´ì—‡","ê·¸ë˜ë„","ë˜ëŠ”","ë§Œ","ë¼ë„","ê¹Œì§€ë„","ì—ì„œë§Œ","ë¶€í„°ë„","ì—ëŠ”","ì´ë©°","ì´ë‚˜","ë¼ë„ìš”",
    "ã…‹ã…‹","ã…ã…","ã… ã… ","ã…œã…œ","ã… ","ã…œ"
}

# ============================ í°íŠ¸ ê²½ë¡œ(ìºì‹œ) ============================
@st.cache_resource(show_spinner=False)
def get_korean_font_path() -> str:
    """ë‚˜ëˆ”ê³ ë”•/ë…¸í† ì‚°ìŠ¤KR ì¤‘ ê°€ëŠ¥í•œ í°íŠ¸ë¥¼ ë‚´ë ¤ë°›ì•„ ì„ì‹œ í´ë”ì— ì €ì¥í•˜ê³  ê²½ë¡œë¥¼ ëŒë ¤ì¤˜ìš”. (ì¸ì ì—†ìŒ)"""
    # 1) ë¡œì»¬ì— í•œê¸€ í°íŠ¸ê°€ ì´ë¯¸ ìˆìœ¼ë©´ ê·¸ê±¸ ì‚¬ìš©
    candidates_local = ["Malgun Gothic", "AppleGothic", "NanumGothic", "NanumSquare", "Noto Sans CJK KR", "Noto Sans KR"]
    for f in font_manager.findSystemFonts(fontpaths=None, fontext="ttf") + font_manager.findSystemFonts(fontpaths=None, fontext="otf"):
        try:
            p = font_manager.FontProperties(fname=f)
            name = font_manager.get_font(f).family_name
        except Exception:
            continue
        if any(k in (name or "") for k in candidates_local):
            return f

    # 2) ì›¹ì—ì„œ ë‚´ë ¤ë°›ê¸°(ì—¬ëŸ¬ URL í›„ë³´ë¥¼ ìˆœì°¨ ì‹œë„)
    urls = [
        # Noto Sans KR (ê³µì‹ ì €ì¥ì†Œ raw)
        "https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTF/Korean/NotoSansKR-Regular.otf",
        # NanumGothic (ë„¤ì´ë²„ ë°°í¬)
        "https://github.com/naver/nanumfont/releases/download/VER2.5/NanumGothic.ttf",
        # Noto Sans KR ë˜ ë‹¤ë¥¸ ê°€ì¤‘ì¹˜
        "https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTF/Korean/NotoSansKR-Medium.otf",
    ]
    session = get_session()
    tmp_dir = tempfile.gettempdir()
    for u in urls:
        filename = os.path.basename(u.split("?")[0]) or "font.otf"
        font_path = os.path.join(tmp_dir, filename)
        try:
            if not os.path.exists(font_path) or os.path.getsize(font_path) < 50000:
                r = session.get(u, timeout=30)
                r.raise_for_status()
                if len(r.content) < 50000:
                    continue
                with open(font_path, "wb") as f:
                    f.write(r.content)
            # ê°„ë‹¨ ê²€ì¦: matplotlibì´ ì½ì„ ìˆ˜ ìˆë‚˜ í™•ì¸
            _ = font_manager.get_font(font_path)
            return font_path
        except Exception:
            continue
    return ""  # ëª¨ë‘ ì‹¤íŒ¨


# ============================ í™”ë©´ êµ¬ì„± ============================
api_key = st.secrets.get("youtube_api_key", "")
if not api_key:
    st.error("ğŸ” API í‚¤ê°€ ì—†ì–´ìš”. .streamlit/secrets.tomlì— ë„£ì–´ ì£¼ì„¸ìš”.")
url = st.text_input("ğŸ“® ìœ íŠœë¸Œ ì£¼ì†Œ", value=DEFAULT_URL, placeholder="ì˜ˆ) https://youtu.be/VIDEO_ID ë˜ëŠ” https://www.youtube.com/watch?v=VIDEO_ID")
max_comments = st.slider("ğŸ§² ìµœëŒ€ ëŒ“ê¸€ ìˆ˜ (ì¸ê¸°ìˆœ)", 100, 2000, 500, step=100)
max_words = st.slider("ğŸ§± ì›Œë“œí´ë¼ìš°ë“œ ë‹¨ì–´ ìˆ˜", 20, 200, 100, step=10)

with st.expander("ğŸ§¹ ë¶ˆìš©ì–´ í¸ì§‘ (ì‰¼í‘œë¡œ êµ¬ë¶„í•´ì„œ ì¶”ê°€í•´ìš”)", expanded=False):
    col_a, col_b = st.columns(2)
    with col_a:
        user_stop_en = st.text_area("ğŸ‡ºğŸ‡¸ ì˜ì–´ ë¶ˆìš©ì–´ ì¶”ê°€", placeholder="ex) video, youtube, like")
    with col_b:
        user_stop_ko = st.text_area("ğŸ‡°ğŸ‡· í•œê¸€ ë¶ˆìš©ì–´ ì¶”ê°€", placeholder="ex) ì •ë§, ê·¸ëƒ¥, ì˜ìƒ")

go = st.button("ğŸš€ ì›Œë“œí´ë¼ìš°ë“œ ë§Œë“¤ê¸°", disabled=(not bool(api_key)))

# ============================ ë™ì‘ ============================
if go:
    video_id = extract_video_id(url)
    if not video_id:
        st.error("â— ì£¼ì†Œê°€ ì˜¬ë°”ë¥´ì§€ ì•Šì•„ìš”.")
        st.stop()

    session = get_session()

    # ì˜ìƒ ì œëª©
    try:
        with st.spinner("ğŸ” ì˜ìƒ ì •ë³´ë¥¼ í™•ì¸í•˜ê³  ìˆì–´ìš”..."):
            title = fetch_video_title(api_key, video_id, session)
    except Exception:
        st.error("âŒ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ë¬¸ì œê°€ ìƒê²¼ì–´ìš”. ì£¼ì†Œì™€ í‚¤ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ ì£¼ì„¸ìš”.")
        st.stop()

    # ëŒ“ê¸€ ìˆ˜ì§‘
    try:
        with st.spinner("ğŸ’¬ ëŒ“ê¸€ì„ ëª¨ìœ¼ëŠ” ì¤‘ì´ì—ìš”... (ì¸ê¸°ìˆœ)"):
            texts = fetch_comment_texts(api_key, video_id, session, max_comments)
    except Exception:
        st.error("âŒ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ë¬¸ì œê°€ ìƒê²¼ì–´ìš”. ì£¼ì†Œì™€ í‚¤ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ ì£¼ì„¸ìš”.")
        st.stop()

    if not texts:
        st.warning("ğŸª« ë¶„ì„ì„ ì§„í–‰í•  ë§Œí¼ì˜ ë‹¨ì–´ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”.")
        st.stop()

    # í† í°í™” + ë¶ˆìš©ì–´
    with st.spinner("ğŸ§ª í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•˜ëŠ” ì¤‘ì´ì—ìš”..."):
        tokens_all: list[str] = []
        for t in texts:
            tokens_all.extend(tokenize_clean(t))

        stop_en = set(WC_STOPWORDS)
        if user_stop_en:
            stop_en |= {w.strip().lower() for w in user_stop_en.split(",") if w.strip()}

        stop_ko = set(w.lower() for w in BASE_KO_STOPWORDS)
        if user_stop_ko:
            stop_ko |= {w.strip().lower() for w in user_stop_ko.split(",") if w.strip()}

        all_stop = stop_en | stop_ko
        tokens_valid = [w for w in tokens_all if w not in all_stop and len(w) >= 2]

    if not tokens_valid:
        st.info("â„¹ï¸ ë¶„ì„ì„ ì§„í–‰í•  ë§Œí¼ì˜ ë‹¨ì–´ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”.")
        st.stop()

    freq = Counter(tokens_valid)
    if not freq:
        st.info("â„¹ï¸ ë¶„ì„ì„ ì§„í–‰í•  ë§Œí¼ì˜ ë‹¨ì–´ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”.")
        st.stop()

    most = dict(freq.most_common(max_words))

    # í°íŠ¸ ì¤€ë¹„(ì—¬ëŸ¬ ê²½ë¡œ ì‹œë„ í›„ ì‹¤íŒ¨ ì‹œ ì•ˆë‚´)
    with st.spinner("ğŸ”¤ í•œê¸€ í°íŠ¸ë¥¼ ì¤€ë¹„í•˜ëŠ” ì¤‘ì´ì—ìš”..."):
        font_path = get_korean_font_path()
        if not font_path or not os.path.exists(font_path):
            st.error("âš ï¸ í°íŠ¸ë¥¼ ë‚´ë ¤ë°›ì§€ ëª»í–ˆì–´ìš”. í•œê¸€ì´ ê¹¨ì§ˆ ìˆ˜ ìˆì–´ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒëµí• ê²Œìš”.")
            st.stop()

    # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
    with st.spinner("ğŸ¨ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ë§Œë“œëŠ” ì¤‘ì´ì—ìš”..."):
        wc = WordCloud(
            width=1200,
            height=600,
            background_color="white",
            font_path=font_path,
            max_words=max_words,
            collocations=False,
            prefer_horizontal=0.9,
            regexp=r"[ê°€-í£a-z0-9]+",
        ).generate_from_frequencies(most)

        fig = plt.figure(figsize=(12, 6), dpi=150)
        plt.imshow(wc, interpolation="bilinear")
        plt.axis("off")
        plt.tight_layout(pad=0)

    st.success("âœ… ì›Œë“œí´ë¼ìš°ë“œë¥¼ ë§Œë“¤ì—ˆì–´ìš”!")
    st.image(wc.to_array(), use_column_width=True, caption="â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ")

    buf = BytesIO()
    fig.savefig(buf, format="png", bbox_inches="tight", pad_inches=0)
    buf.seek(0)
    safe_name = sanitize_filename(title)
    st.download_button(
        "â¬‡ï¸ PNGë¡œ ë‚´ë ¤ë°›ê¸°",
        data=buf,
        file_name=f"{safe_name}_wordcloud.png",
        mime="image/png",
    )
