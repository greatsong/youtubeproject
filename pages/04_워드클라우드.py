# app.py
import streamlit as st
import requests
from urllib.parse import urlparse, parse_qs
import re
from collections import Counter
from io import BytesIO
import tempfile
import os
import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS as WC_STOPWORDS

# ============================ í˜ì´ì§€ ì„¤ì • ============================
st.set_page_config(page_title="ğŸˆ ìœ íŠœë¸Œ ëŒ“ê¸€ ì›Œë“œí´ë¼ìš°ë“œ", layout="wide")
st.title("ğŸˆ ìœ íŠœë¸Œ ëŒ“ê¸€ ì›Œë“œí´ë¼ìš°ë“œ")
st.caption("ğŸ§© ì£¼ì†Œë¥¼ ë„£ê³  ìµœëŒ€ ëŒ“ê¸€ ìˆ˜ë¥¼ ê³ ë¥¸ ë’¤ ë²„íŠ¼ì„ ëˆŒëŸ¬ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ë§Œë“¤ì–´ë´ìš”.")

DEFAULT_URL = "https://www.youtube.com/watch?v=WXuK6gekU1Y"

# ============================ ìœ í‹¸ í•¨ìˆ˜ë“¤ ============================
def extract_video_id(url: str):
    """ì—¬ëŸ¬ í˜•íƒœì˜ ìœ íŠœë¸Œ ì£¼ì†Œì—ì„œ ì˜ìƒ IDë¥¼ ë½‘ì•„ìš”."""
    if not url:
        return None
    url = url.strip()
    if not re.match(r"^https?://", url, flags=re.I):
        url = "https://" + url
    try:
        parsed = urlparse(url)
    except Exception:
        return None

    host = (parsed.netloc or "").lower()
    path = parsed.path or ""
    qs = parse_qs(parsed.query or "")

    if "youtube.com" in host:
        if "v" in qs and qs["v"]:
            return qs["v"][0]
        m = re.search(r"/embed/([A-Za-z0-9_-]{6,})", path)
        if m:
            return m.group(1)
        m = re.search(r"/shorts/([A-Za-z0-9_-]{6,})", path)
        if m:
            return m.group(1)
        m = re.search(r"/live/([A-Za-z0-9_-]{6,})", path)
        if m:
            return m.group(1)
    if "youtu.be" in host:
        m = re.match(r"^/([A-Za-z0-9_-]{6,})", path)
        if m:
            return m.group(1)
    if "vi" in qs and qs["vi"]:
        return qs["vi"][0]
    return None


@st.cache_resource(show_spinner=False)
def get_session() -> requests.Session:
    """API í˜¸ì¶œìš© ì„¸ì…˜ì„ í•œ ë²ˆ ë§Œë“¤ê³  ì¬ì‚¬ìš©í•´ìš”."""
    s = requests.Session()
    s.headers.update({"Accept": "application/json"})
    return s


def raise_api_error(resp: requests.Response) -> None:
    """API ì˜¤ë¥˜ë¥¼ ì‚¬ëŒì´ ì´í•´í•˜ê¸° ì‰¬ìš´ ì˜ˆì™¸ë¡œ ë°”ê¿”ìš”."""
    try:
        data = resp.json()
        reason = data.get("error", {}).get("errors", [{}])[0].get("reason", "")
        code = data.get("error", {}).get("code", resp.status_code)
    except Exception:
        reason = ""
        code = resp.status_code
    raise RuntimeError(f"{code}:{reason}")


def fetch_video_title(api_key: str, video_id: str, session: requests.Session) -> str:
    """ì˜ìƒ ì œëª©ì„ ê°€ì ¸ì™€ìš”(íŒŒì¼ëª…ì— ì“¸ ê±°ì˜ˆìš”)."""
    url = "https://www.googleapis.com/youtube/v3/videos"
    params = {"id": video_id, "part": "snippet", "key": api_key}
    resp = session.get(url, params=params, timeout=20)
    if resp.status_code != 200:
        raise_api_error(resp)
    items = resp.json().get("items", [])
    if not items:
        raise RuntimeError("404:notFound")
    return items[0]["snippet"]["title"] or "video"


def fetch_comment_texts(api_key: str, video_id: str, session: requests.Session, max_count: int) -> list[str]:
    """ì¸ê¸°ìˆœìœ¼ë¡œ ëŒ“ê¸€ ë³¸ë¬¸ì„ ìµœëŒ€ max_countê°œ ëª¨ì•„ìš”."""
    url = "https://www.googleapis.com/youtube/v3/commentThreads"
    params = {
        "part": "snippet",
        "videoId": video_id,
        "key": api_key,
        "order": "relevance",
        "maxResults": 100,
        "textFormat": "plainText",
    }
    texts: list[str] = []
    page_token = None
    while True:
        if page_token:
            params["pageToken"] = page_token
        else:
            params.pop("pageToken", None)
        resp = session.get(url, params=params, timeout=20)
        if resp.status_code != 200:
            raise_api_error(resp)
        data = resp.json()
        for it in data.get("items", []):
            try:
                sn = it["snippet"]["topLevelComment"]["snippet"]
                txt = (sn.get("textDisplay", "") or "").replace("\n", " ").strip()
                if txt:
                    texts.append(txt)
            except Exception:
                continue
            if len(texts) >= max_count:
                return texts
        page_token = data.get("nextPageToken")
        if not page_token:
            break
    return texts


def sanitize_filename(name: str) -> str:
    """íŒŒì¼ëª…ì— ì“¸ ìˆ˜ ì—†ëŠ” ë¬¸ìë¥¼ ì œê±°í•´ìš”."""
    name = re.sub(r"[\\/*?:\"<>|]", "", name)
    name = name.strip() or "wordcloud"
    return name


def tokenize_clean(text: str) -> list[str]:
    """íŠ¹ìˆ˜ë¬¸ì/ì´ëª¨ì§€ ì œê±°, 2ê¸€ì ì´ìƒ í•œ/ì˜ ë‹¨ì–´ë§Œ ë‚¨ê²¨ í† í° ë¦¬ìŠ¤íŠ¸ë¡œ ëŒë ¤ì¤˜ìš”."""
    # ì´ëª¨ì§€/íŠ¹ìˆ˜ë¬¸ì ì œê±°ë¥¼ ìœ„í•´ í•œê¸€/ì˜ë¬¸/ìˆ«ìì™€ ê³µë°±ë§Œ ë‚¨ê²¨ìš”.
    cleaned = re.sub(r"[^ê°€-í£A-Za-z0-9\s]", " ", text)
    # ì†Œë¬¸ì ë³€í™˜
    cleaned = cleaned.lower()
    # ë‹¨ì–´ ë½‘ê¸°(í•œê¸€/ì˜ë¬¸/ìˆ«ì ì¡°í•©)
    tokens = re.findall(r"[ê°€-í£a-z0-9]+", cleaned)
    # 2ê¸€ì ì´ìƒë§Œ ì‚¬ìš©
    tokens = [t for t in tokens if len(t) >= 2]
    return tokens


# --------- í•œê¸€ ê¸°ë³¸ ë¶ˆìš©ì–´(ë„‰ë„‰í•˜ê²Œ, ì¡°ì‚¬/ëŒ€ëª…ì‚¬/ì¶”ì„ìƒˆ/ìƒíˆ¬ì–´ í¬í•¨) ----------
BASE_KO_STOPWORDS = {
    "ê·¸ë¦¬ê³ ","ê·¸ëŸ¬ë‚˜","í•˜ì§€ë§Œ","ê·¸ë˜ì„œ","ë˜í•œ","ë°","ë“±","ë˜","ì•„ë‹ˆë¼","ë³´ë‹¤","ìœ„í•´","ëŒ€í•œ","ë•Œë¬¸","ë•Œë¬¸ì—","ìœ¼ë¡œ","ìœ¼ë¡œì¨","ìœ¼ë¡œì„œ","ì—ì„œ",
    "ì—ê²Œ","ì—ê²Œì„œ","ë¶€í„°","ê¹Œì§€","ì´ë‹¤","ë˜ë‹¤","í•˜ë‹¤","í•©ë‹ˆë‹¤","í•´ìš”","í•©ë‹ˆë‹¤ìš”","í•œë‹¤","í–ˆë‹¤","í•˜ëŠ”","í•˜ë©´","í•˜ë©°","í•˜ì—¬","í•˜ë‹ˆ","í•˜ê³ ",
    "ë©ë‹ˆë‹¤","ë˜ëŠ”","ë˜ì–´","ëë‹¤","ìˆë‹¤","ì—†ë‹¤","ê°™ë‹¤","ìˆ˜","ê²ƒ","ê±°","ë“¤","ê·¸","ì´","ì €","ê·¸ê²ƒ","ì´ê²ƒ","ì €ê²ƒ","ë•Œ","ì¢€","ì•„ì£¼","ë„ˆë¬´","ë§¤ìš°",
    "ì§„ì§œ","ì •ë§","ê·¸ëƒ¥","ì•„ë§ˆ","ì´ë¯¸","ë‹¤ì‹œ","ë‹¤ë¥¸","ìµœê·¼","ì²˜ëŸ¼","ê°™ì´","ìš°ë¦¬","ì €í¬","ë‚´","ë‚´ê°€","ë‚´ê°€ìš”","ë‚˜","ë„ˆ","ë‹ˆ","ë‹ˆê°€","ë„ˆê°€","ê·¸ë…€",
    "ê·¸ëŠ”","ê·¸ë…€ëŠ”","ì €ëŠ”","ë‚˜ëŠ”","ìš°ë¦¬ëŠ”","ì—¬ëŸ¬ë¶„","ì˜¤ëŠ˜","ì˜ìƒ","ëŒ“ê¸€","ìœ íŠœë¸Œ","ë³´ê¸°","ìš”","ë„¤","ì£ ","ìš”ì¦˜","ê±°ì˜","í˜„ì¬","ê·¸ê²Œ","ì´ê²Œ","ì €ê²Œ",
    "ë­”ê°€","ë­","ë­”","ì´ëŸ°","ì €ëŸ°","ê·¸ëŸ°","ë­”ì§€","ì–´ë–¤","ë¬´ì—‡","ê·¸ë˜ë„","ë˜ëŠ”","ë§Œ","ë¼ë„","ê¹Œì§€ë„","ì—ì„œë§Œ","ë¶€í„°ë„","ì—ëŠ”","ì´ë©°","ì´ë‚˜","ë¼ë„ìš”",
    "ã…‹ã…‹","ã…ã…","ã… ã… ","ã…œã…œ","ã… ","ã…œ"
}

# ============================ í°íŠ¸ ê²½ë¡œ(ìºì‹œ) ============================
@st.cache_resource(show_spinner=False)
def get_korean_font_path() -> str:
    """ë‚˜ëˆ”ê³ ë”• ì›¹ì—ì„œ ë°›ì•„ ì„ì‹œ í´ë”ì— ì €ì¥í•˜ê³ , ê²½ë¡œë¥¼ ëŒë ¤ì¤˜ìš”. (ì¸ì ì—†ìŒ)"""
    # ë„¤ì´ë²„ ë‚˜ëˆ”ê³ ë”• ë°°í¬ íŒŒì¼(ì›¹ í˜¸ìŠ¤íŒ… ê²½ë¡œ ì¤‘ í•˜ë‚˜)
    url = "https://github.com/naver/nanumfont/releases/download/VER2.5/NanumGothic.ttf"
    try:
        tmp_dir = tempfile.gettempdir()
        font_path = os.path.join(tmp_dir, "NanumGothic.ttf")
        if not os.path.exists(font_path):
            r = requests.get(url, timeout=30)
            r.raise_for_status()
            with open(font_path, "wb") as f:
                f.write(r.content)
        return font_path
    except Exception:
        return ""  # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´

# ============================ í™”ë©´ êµ¬ì„± ============================
api_key = st.secrets.get("youtube_api_key", "")
if not api_key:
    st.error("ğŸ” API í‚¤ê°€ ì—†ì–´ìš”. .streamlit/secrets.tomlì— ë„£ì–´ ì£¼ì„¸ìš”.")
url = st.text_input("ğŸ“® ìœ íŠœë¸Œ ì£¼ì†Œ", value=DEFAULT_URL, placeholder="ì˜ˆ) https://youtu.be/VIDEO_ID ë˜ëŠ” https://www.youtube.com/watch?v=VIDEO_ID")
max_comments = st.slider("ğŸ§² ìµœëŒ€ ëŒ“ê¸€ ìˆ˜ (ì¸ê¸°ìˆœ)", 100, 2000, 500, step=100)
max_words = st.slider("ğŸ§± ì›Œë“œí´ë¼ìš°ë“œ ë‹¨ì–´ ìˆ˜", 20, 200, 100, step=10)

with st.expander("ğŸ§¹ ë¶ˆìš©ì–´ í¸ì§‘ (ì‰¼í‘œë¡œ êµ¬ë¶„í•´ì„œ ì¶”ê°€í•´ìš”)", expanded=False):
    col_a, col_b = st.columns(2)
    with col_a:
        user_stop_en = st.text_area("ğŸ‡ºğŸ‡¸ ì˜ì–´ ë¶ˆìš©ì–´ ì¶”ê°€", placeholder="ex) video, youtube, like")
    with col_b:
        user_stop_ko = st.text_area("ğŸ‡°ğŸ‡· í•œê¸€ ë¶ˆìš©ì–´ ì¶”ê°€", placeholder="ex) ì •ë§, ê·¸ëƒ¥, ì˜ìƒ")

go = st.button("ğŸš€ ì›Œë“œí´ë¼ìš°ë“œ ë§Œë“¤ê¸°", disabled=(not bool(api_key)))

# ============================ ë™ì‘ ============================
if go:
    # ì£¼ì†Œ ê²€ì‚¬
    video_id = extract_video_id(url)
    if not video_id:
        st.error("â— ì£¼ì†Œê°€ ì˜¬ë°”ë¥´ì§€ ì•Šì•„ìš”.")
        st.stop()

    session = get_session()

    # ì˜ìƒ ì œëª© ê°€ì ¸ì˜¤ê¸° (íŒŒì¼ëª…ì— ì‚¬ìš©)
    try:
        with st.spinner("ğŸ” ì˜ìƒ ì •ë³´ë¥¼ í™•ì¸í•˜ê³  ìˆì–´ìš”..."):
            title = fetch_video_title(api_key, video_id, session)
    except Exception:
        st.error("âŒ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ë¬¸ì œê°€ ìƒê²¼ì–´ìš”. ì£¼ì†Œì™€ í‚¤ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ ì£¼ì„¸ìš”.")
        st.stop()

    # ëŒ“ê¸€ ëª¨ìœ¼ê¸°
    try:
        with st.spinner("ğŸ’¬ ëŒ“ê¸€ì„ ëª¨ìœ¼ëŠ” ì¤‘ì´ì—ìš”... (ì¸ê¸°ìˆœ)"):
            texts = fetch_comment_texts(api_key, video_id, session, max_comments)
    except RuntimeError as e:
        msg = str(e)
        # ê°€ëŠ¥í•œ ìì„¸í•œ ì•ˆë‚´ëŠ” ìƒëµí•˜ê³ , ìš”êµ¬ëœ ê³µí†µ ë¬¸êµ¬ë¡œ ì•ˆë‚´
        st.error("âŒ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ë¬¸ì œê°€ ìƒê²¼ì–´ìš”. ì£¼ì†Œì™€ í‚¤ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ ì£¼ì„¸ìš”.")
        st.stop()
    except Exception:
        st.error("âŒ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ë¬¸ì œê°€ ìƒê²¼ì–´ìš”. ì£¼ì†Œì™€ í‚¤ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ ì£¼ì„¸ìš”.")
        st.stop()

    if not texts:
        st.warning("ğŸª« ë¶„ì„ì„ ì§„í–‰í•  ë§Œí¼ì˜ ë‹¨ì–´ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”.")
        st.stop()

    # í† í°í™” ë° ë¶ˆìš©ì–´ ì²˜ë¦¬
    with st.spinner("ğŸ§ª í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•˜ëŠ” ì¤‘ì´ì—ìš”..."):
        tokens_all: list[str] = []
        for t in texts:
            tokens_all.extend(tokenize_clean(t))

        # ì˜ì–´ ê¸°ë³¸ ë¶ˆìš©ì–´ + ì‚¬ìš©ì ì¶”ê°€
        stop_en = set(WC_STOPWORDS)
        if user_stop_en:
            stop_en |= {w.strip().lower() for w in user_stop_en.split(",") if w.strip()}

        # í•œê¸€ ê¸°ë³¸ ë¶ˆìš©ì–´ + ì‚¬ìš©ì ì¶”ê°€
        stop_ko = set(w.lower() for w in BASE_KO_STOPWORDS)
        if user_stop_ko:
            stop_ko |= {w.strip().lower() for w in user_stop_ko.split(",") if w.strip()}

        all_stop = stop_en | stop_ko

        tokens_valid = [w for w in tokens_all if w not in all_stop and len(w) >= 2]

    if not tokens_valid:
        st.info("â„¹ï¸ ë¶„ì„ì„ ì§„í–‰í•  ë§Œí¼ì˜ ë‹¨ì–´ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”.")
        st.stop()

    # ë¹ˆë„ ê³„ì‚°
    freq = Counter(tokens_valid)
    if not freq:
        st.info("â„¹ï¸ ë¶„ì„ì„ ì§„í–‰í•  ë§Œí¼ì˜ ë‹¨ì–´ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”.")
        st.stop()

    # ìƒìœ„ max_wordsë§Œ ì‚¬ìš©
    most = dict(freq.most_common(max_words))

    # í°íŠ¸ ì¤€ë¹„
    with st.spinner("ğŸ”¤ í•œê¸€ í°íŠ¸ë¥¼ ì¤€ë¹„í•˜ëŠ” ì¤‘ì´ì—ìš”..."):
        font_path = get_korean_font_path()
        if not font_path or not os.path.exists(font_path):
            st.warning("âš ï¸ í°íŠ¸ë¥¼ ë‚´ë ¤ë°›ì§€ ëª»í–ˆì–´ìš”. í•œê¸€ì´ ê¹¨ì§ˆ ìˆ˜ ìˆì–´ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ìƒëµí• ê²Œìš”.")
            st.stop()

    # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
    with st.spinner("ğŸ¨ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ë§Œë“œëŠ” ì¤‘ì´ì—ìš”..."):
        wc = WordCloud(
            width=1200,
            height=600,
            background_color="white",
            font_path=font_path,
            max_words=max_words,
            collocations=False,
            prefer_horizontal=0.9,
            regexp=r"[ê°€-í£a-z0-9]+",
        ).generate_from_frequencies(most)

        fig = plt.figure(figsize=(12, 6), dpi=150)
        plt.imshow(wc, interpolation="bilinear")
        plt.axis("off")
        plt.tight_layout(pad=0)

    st.success("âœ… ì›Œë“œí´ë¼ìš°ë“œë¥¼ ë§Œë“¤ì—ˆì–´ìš”!")
    st.image(wc.to_array(), use_column_width=True, caption="â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ")

    # PNG ë‹¤ìš´ë¡œë“œ
    buf = BytesIO()
    fig.savefig(buf, format="png", bbox_inches="tight", pad_inches=0)
    buf.seek(0)
    safe_name = sanitize_filename(title)
    st.download_button(
        "â¬‡ï¸ PNGë¡œ ë‚´ë ¤ë°›ê¸°",
        data=buf,
        file_name=f"{safe_name}_wordcloud.png",
        mime="image/png",
    )
