# app.py
import streamlit as st
import requests
from urllib.parse import urlparse, parse_qs
import re
from collections import Counter
from io import BytesIO
from datetime import datetime
from pathlib import Path
from wordcloud import WordCloud
import matplotlib.pyplot as plt

# ---------------------------- ê¸°ë³¸ ì„¤ì • ----------------------------
st.set_page_config(page_title="ìœ íŠœë¸Œ ëŒ“ê¸€ ì›Œë“œí´ë¼ìš°ë“œ", layout="wide")
st.title("ìœ íŠœë¸Œ ëŒ“ê¸€ ì›Œë“œí´ë¼ìš°ë“œ")
st.caption("ì£¼ì†Œì™€ ìµœëŒ€ ëŒ“ê¸€ ìˆ˜ë¥¼ ì •í•˜ê³ , ì¸ê¸°ìˆœ ëŒ“ê¸€ë¡œ ì›Œë“œí´ë¼ìš°ë“œë¥¼ ë§Œë“¤ì–´ìš”.")

DEFAULT_URL = "https://www.youtube.com/watch?v=WXuK6gekU1Y"

# ---------------------------- í°íŠ¸(ë‚˜ëˆ”ê³ ë”•) ì¤€ë¹„ ----------------------------
@st.cache_resource(show_spinner=False)
def get_nanumgothic_path() -> str | None:
    """ì•±ì´ ì²˜ìŒ ì‹¤í–‰ë  ë•Œ ì›¹ì—ì„œ ë‚˜ëˆ”ê³ ë”• í°íŠ¸ë¥¼ ë°›ì•„ì™€ ì„ì‹œ í´ë”ì— ì €ì¥í•´ìš”."""
    url = "https://github.com/google/fonts/raw/main/ofl/nanumgothic/NanumGothic-Regular.ttf"
    dest = Path(st.experimental_get_query_params().get("_cache_dir", [str(Path.cwd() / ".tmp")])[0])
    dest.mkdir(parents=True, exist_ok=True)
    fp = dest / "NanumGothic-Regular.ttf"
    try:
        if not fp.exists() or fp.stat().st_size == 0:
            resp = requests.get(url, timeout=20)
            if resp.status_code == 200:
                fp.write_bytes(resp.content)
        return str(fp) if fp.exists() and fp.stat().st_size > 0 else None
    except Exception:
        return None

FONT_PATH = get_nanumgothic_path()

# ---------------------------- ìœ í‹¸ë¦¬í‹° ----------------------------
def extract_video_id(url: str) -> str | None:
    """ì—¬ëŸ¬ í˜•íƒœì˜ ìœ íŠœë¸Œ ì£¼ì†Œ(ì¼ë°˜/ì§§ì€/shorts/embed/live ë“±)ì—ì„œ ì˜ìƒ IDë¥¼ ë½‘ì•„ì¤˜ìš”."""
    if not url:
        return None
    url = url.strip()
    if not re.match(r"^https?://", url, flags=re.I):
        url = "https://" + url
    try:
        parsed = urlparse(url)
    except Exception:
        return None

    host = (parsed.netloc or "").lower()
    path = parsed.path or ""
    qs = parse_qs(parsed.query or "")

    if "youtube.com" in host:
        if "v" in qs and qs["v"]:
            return qs["v"][0]
        m = re.search(r"/embed/([A-Za-z0-9_-]{6,})", path)
        if m:
            return m.group(1)
        m = re.search(r"/shorts/([A-Za-z0-9_-]{6,})", path)
        if m:
            return m.group(1)
        m = re.search(r"/live/([A-Za-z0-9_-]{6,})", path)
        if m:
            return m.group(1)
    if "youtu.be" in host:
        m = re.match(r"^/([A-Za-z0-9_-]{6,})", path)
        if m:
            return m.group(1)
    if "vi" in qs and qs["vi"]:
        return qs["vi"][0]
    return None


def raise_api_error(resp: requests.Response) -> None:
    """API ì˜¤ë¥˜ë¥¼ ì½ì–´ì„œ ì˜ˆì™¸ë¡œ ì˜¬ë ¤ì¤˜ìš”."""
    try:
        data = resp.json()
        reason = data.get("error", {}).get("errors", [{}])[0].get("reason", "")
        code = data.get("error", {}).get("code", resp.status_code)
    except Exception:
        reason = ""
        code = resp.status_code
    raise RuntimeError(f"{code}:{reason}")


@st.cache_resource(show_spinner=False)
def get_http_session() -> requests.Session:
    """ìœ íŠœë¸Œ API í˜¸ì¶œìš© ì„¸ì…˜ì„ í•œ ë²ˆ ë§Œë“¤ê³  ì¬ì‚¬ìš©í•´ìš”."""
    s = requests.Session()
    s.headers.update({"Accept": "application/json"})
    return s


def fetch_comments_plaintext(api_key: str, video_id: str, session: requests.Session, limit: int) -> list[str]:
    """ì¸ê¸°ìˆœìœ¼ë¡œ ëŒ“ê¸€ ë³¸ë¬¸ë§Œ ëª¨ì•„ì¤˜ìš”(ìµœëŒ€ limitê°œ)."""
    url = "https://www.googleapis.com/youtube/v3/commentThreads"
    params = {
        "part": "snippet",
        "videoId": video_id,
        "key": api_key,
        "order": "relevance",
        "maxResults": 100,
        "textFormat": "plainText",
    }
    texts: list[str] = []
    page_token = None
    while True:
        if page_token:
            params["pageToken"] = page_token
        else:
            params.pop("pageToken", None)

        resp = session.get(url, params=params, timeout=20)
        if resp.status_code != 200:
            raise_api_error(resp)

        data = resp.json()
        for it in data.get("items", []):
            try:
                sn = it["snippet"]["topLevelComment"]["snippet"]
                txt = (sn.get("textDisplay") or sn.get("textOriginal") or "").replace("\n", " ").strip()
                if txt:
                    texts.append(txt)
            except Exception:
                continue
            if len(texts) >= limit:
                return texts

        page_token = data.get("nextPageToken")
        if not page_token:
            break
    return texts


# ì´ëª¨ì§€ ì œê±°(ì •ê·œì‹ ë²”ìœ„ë¡œ ì²˜ë¦¬)
EMOJI_PATTERN = re.compile(
    "["  # ëŒ€í‘œì ì¸ ì´ëª¨ì§€/ì‹¬ë³¼ ë¸”ë¡ë“¤
    "\U0001F600-\U0001F64F"
    "\U0001F300-\U0001F5FF"
    "\U0001F680-\U0001F6FF"
    "\U0001F1E0-\U0001F1FF"
    "\U00002700-\U000027BF"
    "\U00002600-\U000026FF"
    "\U0001F900-\U0001F9FF"
    "\U0001FA70-\U0001FAFF"
    "\U0001F3FB-\U0001F3FF"
    "]+",
    flags=re.UNICODE,
)

URL_PATTERN = re.compile(r"https?://\S+|www\.\S+", flags=re.I)

def clean_and_tokenize(text: str) -> list[str]:
    """íŠ¹ìˆ˜ë¬¸ìÂ·ì´ëª¨ì§€ ì œê±° í›„, 2ê¸€ì ì´ìƒ í•œê¸€/ì˜ë¬¸ ë‹¨ì–´ë§Œ ì†Œë¬¸ìë¡œ ë½‘ì•„ìš”."""
    t = text.lower()
    t = URL_PATTERN.sub(" ", t)
    t = EMOJI_PATTERN.sub(" ", t)
    # íŠ¹ìˆ˜ë¬¸ìëŠ” ê³µë°±ìœ¼ë¡œ ì¹˜í™˜
    t = re.sub(r"[^ê°€-í£a-z\s]", " ", t)
    # 2ê¸€ì ì´ìƒ í•œê¸€/ì˜ë¬¸ í† í° ì¶”ì¶œ
    tokens = re.findall(r"[ê°€-í£a-z]{2,}", t)
    return tokens


def parse_stopwords(base_csv: str) -> set[str]:
    """ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¶ˆìš©ì–´ ë¬¸ìì—´ì„ ì§‘í•©ìœ¼ë¡œ ë°”ê¿”ì¤˜ìš”(ì†Œë¬¸ì/ê³µë°± ì œê±°)."""
    words = [w.strip().lower() for w in base_csv.split(",") if w.strip()]
    return set(words)


def sanitize_filename(name: str) -> str:
    """íŒŒì¼ ì´ë¦„ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ë¬¸ìë¥¼ ì œê±°í•´ìš”."""
    name = re.sub(r"[^\w\-]+", "_", name)
    return name.strip("_") or "wordcloud"


# ---------------------------- ê¸°ë³¸ ë¶ˆìš©ì–´(í¸ì§‘ ê°€ëŠ¥) ----------------------------
DEFAULT_STOPWORDS = ", ".join([
    # í•œêµ­ì–´ ì˜ˆì‹œ
    "ì´","ê·¸","ì €","ê²ƒ","ìˆ˜","ë“±","ì¢€","ì˜","ë”","ì •ë§","ì§„ì§œ","ë„ˆë¬´","ì™„ì „","ê·¼ë°","ê·¸ë˜ì„œ","ê·¸ë¦¬ê³ ","í•˜ì§€ë§Œ","ì´ì œ","ì˜ìƒ","êµ¬ë…","ì¢‹ì•„ìš”","ã…‹ã…‹","ã…ã…","ã… ã… ","^^",
    # ì˜ì–´ ì˜ˆì‹œ
    "the","a","an","is","are","be","to","of","and","in","that","it","with","for","on","this","i","you","he","she","we","they","my","your","lol","omg","btw",
    # ì´ëª¨ì§€ ì˜ˆì‹œ(ì‰¼í‘œë¡œ ë‚˜ì—´)
    "ğŸ˜‚","ğŸ¤£","ğŸ˜","ğŸ‘","ğŸ™","ğŸ”¥","âœ¨","ğŸ‰","â¤ï¸","ğŸ’¯","ğŸ˜…","ğŸ¥²","ğŸ˜­","ğŸ˜¢","ğŸ‘","ğŸ’–","ğŸ˜","ğŸ˜Š","ğŸ˜‰","ğŸ™Œ",
])

# ---------------------------- í™”ë©´ ----------------------------
api_key = st.secrets.get("youtube_api_key", "")
if not api_key:
    st.error("API í‚¤ê°€ ì—†ì–´ìš”. .streamlit/secrets.tomlì— ë„£ì–´ ì£¼ì„¸ìš”.")

url_input = st.text_input("ìœ íŠœë¸Œ ì£¼ì†Œë¥¼ ë„£ì–´ ì£¼ì„¸ìš”.", value=DEFAULT_URL)
max_comments = st.slider("ìµœëŒ€ ëŒ“ê¸€ ìˆ˜ (100~2000, 100 ë‹¨ìœ„)", min_value=100, max_value=2000, step=100, value=500)
max_words = st.slider("ì›Œë“œí´ë¼ìš°ë“œ ë‹¨ì–´ ìˆ˜ (20~200, 10 ë‹¨ìœ„)", min_value=20, max_value=200, step=10, value=100)

st.write("ë¶ˆìš©ì–´ ëª©ë¡ì„ ì‰¼í‘œë¡œ ìˆ˜ì •/ì¶”ê°€í•  ìˆ˜ ìˆì–´ìš”.")
stopwords_csv = st.text_area("í™•ì¥ ë¶ˆìš©ì–´(ì‰¼í‘œë¡œ êµ¬ë¶„)", value=DEFAULT_STOPWORDS, height=120)

run_btn = st.button("ëŒ“ê¸€ ëª¨ìœ¼ê³  ì›Œë“œí´ë¼ìš°ë“œ ë§Œë“¤ê¸°", disabled=(not bool(api_key)))

# ë²„íŠ¼ì„ ëˆ„ë¥´ê¸° ì „ì—ëŠ” ì–´ë–¤ ì—°ê²°ë„ ì‹œë„í•˜ì§€ ì•Šì•„ìš”(ìœ íŠœë¸Œ API).
if run_btn:
    # ì£¼ì†Œ í™•ì¸
    vid = extract_video_id(url_input)
    if not vid:
        st.error("ì£¼ì†Œê°€ ì˜¬ë°”ë¥´ì§€ ì•Šì•„ìš”.")
        st.stop()

    # ëŒ“ê¸€ ê°€ì ¸ì˜¤ê¸°
    session = get_http_session()
    try:
        texts = fetch_comments_plaintext(api_key, vid, session, max_comments)
    except Exception:
        st.error("ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ë¬¸ì œê°€ ìƒê²¼ì–´ìš”. ì£¼ì†Œì™€ í‚¤ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ ì£¼ì„¸ìš”.")
        st.stop()

    if not texts:
        st.error("ë¶„ì„ì„ ì§„í–‰í•  ë§Œí¼ì˜ ë‹¨ì–´ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”.")
        st.stop()

    # ì „ì²˜ë¦¬ + í† í°í™”
    tokens: list[str] = []
    for t in texts:
        tokens.extend(clean_and_tokenize(t))

    # ë¶ˆìš©ì–´ ì ìš©(ê¸°ë³¸ + ì‚¬ìš©ìê°€ ì…ë ¥í•œ í™•ì¥ ëª©ë¡)
    user_stop = parse_stopwords(stopwords_csv)
    # ì´ëª¨ì§€ëŠ” ì „ì²˜ë¦¬ì—ì„œ ì§€ì›Œì§€ì§€ë§Œ, í˜¹ì‹œ ë‚¨ì•„ìˆì„ ìˆ˜ ìˆì–´ í•¨ê»˜ ì œê±°
    tokens = [w for w in tokens if (len(w) >= 2 and w not in user_stop)]

    if not tokens:
        st.error("ë¶„ì„ì„ ì§„í–‰í•  ë§Œí¼ì˜ ë‹¨ì–´ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”.")
        st.stop()

    # ë¹ˆë„ ê³„ì‚°
    freq = Counter(tokens)
    if not freq:
        st.error("ë¶„ì„ì„ ì§„í–‰í•  ë§Œí¼ì˜ ë‹¨ì–´ë¥¼ ì°¾ì§€ ëª»í–ˆì–´ìš”.")
        st.stop()

    # ì›Œë“œí´ë¼ìš°ë“œ ìƒì„±
    max_words = int(max_words)
    wc = WordCloud(
        font_path=FONT_PATH,              # í•œê¸€ í‘œì‹œë¥¼ ìœ„í•´ í°íŠ¸ ì ìš©
        width=1200,
        height=700,
        background_color="white",
        max_words=max_words,
        collocations=False,
    ).generate_from_frequencies(freq)

    # í™”ë©´ì— ê·¸ë¦¬ê¸°
    fig, ax = plt.subplots(figsize=(12, 7))
    ax.imshow(wc, interpolation="bilinear")
    ax.axis("off")
    st.pyplot(fig, use_container_width=True)

    # PNGë¡œ ë‚´ë ¤ë°›ê¸°
    img = wc.to_image()
    buf = BytesIO()
    img.save(buf, format="PNG")
    png_bytes = buf.getvalue()

    safe_name = sanitize_filename(f"wordcloud_{vid}_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
    st.download_button(
        label="PNGë¡œ ë‚´ë ¤ë°›ê¸°",
        data=png_bytes,
        file_name=f"{safe_name}.png",
        mime="image/png",
    )
